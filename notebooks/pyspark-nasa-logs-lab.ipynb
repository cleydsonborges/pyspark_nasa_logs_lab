{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Nasa Análise de Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import datetime\n",
    "\n",
    "\n",
    "from pyspark.sql import Row\n",
    "\n",
    "month_map = {'Jan': 1, 'Feb': 2, 'Mar':3, 'Apr':4, 'May':5, 'Jun':6, 'Jul':7,\n",
    "    'Aug':8,  'Sep': 9, 'Oct':10, 'Nov': 11, 'Dec': 12}\n",
    "\n",
    "def parse_apache_time(s):\n",
    "    \"\"\" Converte a string de data do log em um objeto datetime do python\n",
    "    Args:\n",
    "        s (str): date e time como string\n",
    "    Returns:\n",
    "        datetime: datetime object (ignora o timezone)\n",
    "    \"\"\"\n",
    "    return datetime.datetime(int(s[7:11]),\n",
    "                             month_map[s[3:6]],\n",
    "                             int(s[0:2]),\n",
    "                             int(s[12:14]),\n",
    "                             int(s[15:17]),\n",
    "                             int(s[18:20]))\n",
    "\n",
    "def parse_apache_log_line(logline):\n",
    "    \"\"\" Faz o parse da linha no formato Apache Common Log\n",
    "    Args:\n",
    "        logline (str): uma linha em texto no formato Apache Common Log\n",
    "    Returns:\n",
    "        tuple: retorna um dicionário com atributos dos logs e status 1 para linhas \n",
    "        que foram parseadas com sucesso e 0 para falhas\n",
    "    \"\"\"\n",
    "    match = re.search(APACHE_ACCESS_LOG_PATTERN, logline)\n",
    "    if match is None:\n",
    "        return (logline, 0)\n",
    "    size_field = match.group(9)\n",
    "    if size_field == '-':\n",
    "        size = int(0)\n",
    "    else:\n",
    "        size = int(match.group(9))\n",
    "    return (Row(\n",
    "        host          = match.group(1),\n",
    "        client_identd = match.group(2),\n",
    "        user_id       = match.group(3),\n",
    "        date_time     = parse_apache_time(match.group(4)),\n",
    "        method        = match.group(5),\n",
    "        endpoint      = match.group(6),\n",
    "        protocol      = match.group(7),\n",
    "        response_code = int(match.group(8)),\n",
    "        content_size  = size\n",
    "    ), 1)\n",
    "\n",
    "# Padrão da expressão para extrair os dados de logs de maneira estruturada.\n",
    "APACHE_ACCESS_LOG_PATTERN = '^(\\S+) (\\S+) (\\S+) \\[([\\w:/]+\\s[+\\-]\\d{4})\\] \"(\\S+) (\\S+)\\s*(\\S*)\" (\\d{3}) (\\S+)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10001 linhas lidas, 10001 linhas parseadas com sucesso, 0 linhas falharam o parse\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext(appName=\"Nasa\")\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "logFile = '../resources'\n",
    "\n",
    "def parseLogs():\n",
    "    \"\"\" Ler o Arquivo e fazer o parse do LOG \"\"\"\n",
    "    parsed_logs = (sc\n",
    "                   .textFile(logFile)\n",
    "                   .map(parse_apache_log_line)\n",
    "                   .cache())\n",
    "\n",
    "    access_logs = (parsed_logs\n",
    "                   .filter(lambda s: s[1] == 1)\n",
    "                   .map(lambda s: s[0])\n",
    "                   .cache())\n",
    "\n",
    "    failed_logs = (parsed_logs\n",
    "                   .filter(lambda s: s[1] == 0)\n",
    "                   .map(lambda s: s[0]))\n",
    "    failed_logs_count = failed_logs.count()\n",
    "    if failed_logs_count > 0:\n",
    "        print('Número de linhas inválidas no log: {}'.format(failed_logs.count()))\n",
    "        for line in failed_logs.take(20):\n",
    "            print('Invalid logline: {}'.format(line))\n",
    "\n",
    "\n",
    "    print('{} linhas lidas, {} linhas parseadas com sucesso, {} linhas falharam o parse'.format(parsed_logs.count(), access_logs.count(), failed_logs.count()))\n",
    "    return parsed_logs, access_logs, failed_logs\n",
    "\n",
    "\n",
    "parsed_logs, access_logs, failed_logs = parseLogs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Número de hosts únicos.: 936\n",
      "2. O total de erros 404.: 48\n",
      "3. As 5 URLs que mais causaram erro 404.: [('/shuttle/missions/sts-71/images/KSC-95EC-0916.txt', 9), ('/pub/winvn/readme.txt', 7), ('/pub/winvn/release.txt', 5), ('/history/apollo/publications/sp-350/sp-350.txt~', 3), ('/shuttle/technology/images/sts-comm-small.gif', 3)]\n",
      "4. Quantidade de erros 404 por dia.: 48\n",
      "5. O total de bytes retornados.: 246479899\n"
     ]
    }
   ],
   "source": [
    "#Questões\n",
    "\n",
    "#1. Número de hosts únicos.\n",
    "hosts = access_logs.map(lambda log: log.host)\n",
    "unique_hosts = hosts.distinct()\n",
    "unique_host_count = unique_hosts.count()\n",
    "print('1. Número de hosts únicos.: {}'.format(unique_host_count))\n",
    "    \n",
    "#2. O total de erros 404.\n",
    "# Usando Dataframe:\n",
    "# logs_df = access_logs.toDF()\n",
    "# not_found_df = logs_df.filter(logs_df['response_code']==404).cache()\n",
    "# print('Total de erros 404: {}'.format(not_found_df.count()))\n",
    "bad_records = access_logs.filter(lambda log: log.response_code==404).cache()\n",
    "print('2. O total de erros 404.: {}'.format(bad_records.count()))\n",
    "\n",
    "\n",
    "#3. As 5 URLs que mais causaram erro 404.\n",
    "err_404 = access_logs.filter(lambda log: log.response_code == 404)\n",
    "endpoint_count_pair_tuple = err_404.map(lambda log: (log.endpoint, 1))\n",
    "endpoint_sum = endpoint_count_pair_tuple.reduceByKey(lambda a, b: a + b)\n",
    "five_top_err_urls = endpoint_sum.takeOrdered(5, lambda s: -1 * s[1])\n",
    "print('3. As 5 URLs que mais causaram erro 404.: {}'.format(five_top_err_urls))\n",
    "    \n",
    "    \n",
    "#4. Quantidade de erros 404 por dia.\n",
    "err_date_count_pair_tuple = bad_records.map(lambda log: (log.date_time.day, 1))\n",
    "err_date_sum = err_date_count_pair_tuple.reduceByKey(lambda a, b: a+b)\n",
    "err_date_sorted = err_date_sum.sortByKey().cache()\n",
    "err_by_date = err_date_sorted.collect()\n",
    "print('4. Quantidade de erros 404 por dia.: {}'.format(err_by_date))\n",
    "    \n",
    "#5. O total de bytes retornados.\n",
    "content_sizes = access_logs.map(lambda log: log.content_size).cache()\n",
    "content_sizes = content_sizes.reduce(lambda a, b : a + b)\n",
    "print('5. O total de bytes retornados.: {}'.format(content_sizes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
